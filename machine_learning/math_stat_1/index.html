<!DOCTYPE html>
<html>
    <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width,initial-scale=1,minimum-scale=1">
    <title>數理統計學 - 點估計 - Little Cube`s box</title>

    <style type="text/css">
        img.wp-smiley,
        img.emoji {
            display: inline !important;
            border: none !important;
            box-shadow: none !important;
            height: 1em !important;
            width: 1em !important;
            margin: 0 .07em !important;
            vertical-align: -0.1em !important;
            background: none !important;
            padding: 0 !important;
        }
    </style>
    <link rel='stylesheet' href='/css/style.css' type='text/css' media='all' />
    <link rel='stylesheet' href='/css/custom.css' type='text/css' media='all' />
    <link rel='stylesheet' href='/css/syntax.css' type='text/css' media='all' />
    <link rel='stylesheet' href='/css/toc.css' type='text/css' media='all' />
    
    <meta property="og:title" content="數理統計學 - 點估計" />
<meta property="og:description" content="點估計、估計式、三大估計方法" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://littlecube2019.github.io/machine_learning/math_stat_1/" />
<meta property="article:published_time" content="2021-04-01T04:00:15+08:00" />
<meta property="article:modified_time" content="2021-04-01T04:00:15+08:00" />

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="數理統計學 - 點估計"/>
<meta name="twitter:description" content="點估計、估計式、三大估計方法"/>

</head>

    <body class="two-column">
        <a href="#content">Skip to content</a>
<div class="wrapper">
    <header role="banner" class="banner widgets columns-1">
        <a href="/" rel="home">
            <h1 class="site">Little Cube`s box</h1>
            
            <p>筆記暫存處</p>

             
                <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        </a>
    

        <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </header>

    <br>
    <div style="width: 100%; max-height: 100px; text-align: center;">
       
</div>

    <div class="breadcrumbs">
        
    </div>
</div>
        <div id="content" class="content">

<main role="main">
    <article role="article" class="post type-post format-standard hentry">
        <header class="post-header">
            <h1>數理統計學 - 點估計</h1>
            <div class="post-details">
                <a rel="bookmark">
                    <time datetime="2021-04-01T41:40:156">2021-04-01</time>
                </a>
				
            </div>
        </header>

        <div class="post-content">
            <p>參考政大<a href="http://ctld.video.nccu.edu.tw/media/721#note-tabs-note">劉惠美老師線上課程</a></p>
<h1 id="點估計">點估計</h1>
<p>統計兩大議題之一(估計與檢定)</p>
<p>定義:<br>
Statistic(統計量): 隨機變數所構成的函數，不包含未知的母數<br>
EX: $\bar{x} \quad \sum_i(x_i) \quad \sum_i{(x_i-\bar{x})}^2$</p>
<p>Estimator(估計式)是一個用來估計未知母數的統計量<br>
Note:通常母數參數用$\theta$表示，$\hat{\theta}$則是他的估計式</p>
<p>Estimate(估計值)是帶入數字的值</p>
<h1 id="方法介紹">方法介紹</h1>
<p>一. 動差估計法 (Method of Moments Estimator) <br>
要先得知母體的分布&quot;種類&rdquo;，才能知道n階動差的格式
進一步令n階樣本動差等於n階母體動差<br>
然後解聯立方程求出母體參數，得到母體確切形狀<br>
(如果母體只有1個參數，那就只需要一階，也就是一個方程式，以此類推)</p>
<p>母體動差:  $E(X^k)$    <br>
樣本動差:  $\sum_{i=1}^n{x_i^k}/n$</p>
<p>很明顯地，樣本動差省略了&quot;各值機率P(X|$\theta$)&ldquo;這個重要的訊息</p>
<p>解釋: 以離散期待值來說，母體動差= $\sum{X^k * P(X|$\theta$)}$，而P(X|$\theta$)是由母體分布得來<br>
而樣本動差則直接視為均值分布</p>
<p>Example:  白努力分布<br>
已知 $X_i$~bernoulli(p) ，並且$x_i$是iid<br>
則 $\hat{E(X)} = \hat{p} = \frac{\sum{x}}{n}$<br>
(做實驗可以得知X_i，就可以帶入估計式得到p的估計值)</p>
<p>Example: 均值分布<br>
已知 $X_i$~U(0,$\theta$) ，並且$x_i$是iid， 服從0~$\theta$<br>
$\hat{E(X)} = \hat{\theta}/2 = \bar{x}$<br>
得  $\hat{\theta} = 2\bar{x} $</p>
<p>Example: 常態分佈<br>
已知 $X_i$~bernoulli($\mu$,$\sigma^2$) ，並且$x_i$是iid<br>
$\hat{E(X)} = \hat{\mu} = \bar{x} $  <br>
$ \hat{E(X^2)} = \hat{\sigma^2} - \hat{\mu^2} = \sum{x^2}/n $</p>
<p>Note:</p>
<ol>
<li>
<p>估計出來的$\sigma$會有bias，所以需要乘上(n-1)/n去調整<br>
(因為$\sigma$跟sample版的差了n-1，但樣本動差分母是除n )</p>
</li>
<li>
<p>注意二階動差 =&gt; Var(X) - $\mu^2$ ，這個格式常常用到</p>
</li>
</ol>
<p>優點: 方便計算<br>
缺點: 可能會失準</p>
<p>解釋: 以上例來說，若以U(0,1)為實驗，發現抽樣$\bar{x}$=0.64，則$\hat{\theta}$=1.28</p>
<p>二. 最大概似法 (Maximum Likelihood Estimator) <br>
定義: Likelihood(function)<br>
與原本的pdf一模一樣，只是給定變成隨機變數X，未知變成參數</p>
<p>Probability 是 給定參數 =&gt; 求隨機變數X發生的機率        <br>
Likelihood  是 給定某些可能值 =&gt; 推得參數</p>
<p>Example:<br>
Probability:<br>
已知洋芋片一包重量成常態分佈N($\mu$,$\sigma$)，求位於a克重的機率<br>
寫成數學表達式為<br>
$$ P(X=a|\mu,\sigma) $$</p>
<p>Likelihood:<br>
已知洋芋片有a克重，求其在常態分佈$\mu$,$\sigma$下的機率<br>
寫成數學表達式為<br>
$$ L(\mu,\sigma | X=a)$$</p>
<p>可以知道這<b style="color:red; " >兩者的結果都是機率值，只是已知與待求相反</b></p>
<div style="text-align:center">
<img src= "\machine_learning\math_stat_1-1.PNG" style="width:800px;height:350px">
</div>
<p><a href="https://www.youtube.com/watch?v=pYxNSUDSFH4">也可以參考statQuest Probability Vs Likelihood</a><br>
<a href="https://www.youtube.com/watch?v=Dn6b9fCIUpM">statQuest-MLE</a></p>
<p>MLE就只是 <b style="color:red; " >求一個特定分布，使觀察到的特定點機率最大</b><br>
而最大值求法就使用<b style="color:red; " >一皆偏微分=0</b> 即可<br>
並且技巧上會先取log</p>
<p>Example:  常態分佈MLE
有三個符合常態分布N($\mu$,$\sigma$)的iid隨機變數<br>
其聯合機率分布為<br>
$$\Pi_{i=1,2,3} \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x_i-\mu)^2}{2\sigma^2}} =\frac{1}{(2\pi\sigma^2)^{3/2}}e^{-\sum_{i=1,2,3}\frac{(x_i-\mu)^2}{2\sigma^2}}  $$</p>
<p>Likelihood一樣是使用這個分布公式，只是我們變成已知X1,X2,X3，要去找$\mu$,$\sigma$<br>
MLE則是要找到使X1~X3發生機率最大的$\mu$,$\sigma$</p>
<p>作法:取log後偏微分=0 <br>
求$\mu$<br>
$$\frac{\partial}{\partial \mu}(-3/2)ln(2\pi\sigma^2)-\sum_{i=1,2,3}\frac{(x_i-\mu)^2}{2\sigma^2}=0$$<br>
經過偏微分得 <br>
$$-\frac{1}{2\sigma^2}\sum_{i=1,2,3}(-1)2(x_i-\mu)=0$$
整理得到<br>
$$\mu = \frac{\sum_{i=1,2,3}x_i}{3} (其實是估計版的\hat{\mu})$$</p>
<p>Note:</p>
<ol>
<li>可得知在常態分佈，MLE與MME結果一樣都是樣本平均值 = 母體平均值</li>
<li>$\hat{\sigma}$如法炮製即可</li>
<li>Beta , Gamma 分布由於導數不好求，通常都用MME估計</li>
</ol>
<p>Example:  Poissson MLE<br>
令n個iid的X_i隨機變數服從possion($\theta$)<br>
則其聯合機率分布函數為<br>
$$L(\theta) = \Pi_{i=1~n}\frac{\theta^{X_i}e^{-\theta}}{x_i!} = \frac{\theta^{\sum{x_i}}e^{-n\theta}}{\Pi_{i=1~n}X_i!}$$</p>
<p>取log<br>
$$lnL(\theta) =\sum{x_i}ln\theta-n\theta-ln(\Pi_{i=1~n}X_i!)$$</p>
<p>求偏微分=0<br>
$$\frac{\partial lnL(\theta)}{\partial\theta}  = 0 $$
得 $\hat\theta = \bar{X}$ ，也就一樣是樣本平均數</p>
<p>Note:<br>
嚴謹一點的話，MLE一階偏微分完後，還要證明二階&lt;0，才會確定為最大值!</p>
<p>Thm: MLE的不變性 (MLE`s Invariance property)<br>
如果要估計經過MLE得到的估計參數的函式值，只需要把MLE參數代入即可 (非常直覺的定理)<br>
$$\hat{f(\theta)}=f(\hat{\theta})$$</p>
<p>有趣的是，Uniform Distribution無法求偏倒數=0的解，只能證明找到0~$\theta$使L($\theta$)最大<br>
而可以得證<b style="color:red; " >上下界估計值為樣本中的最小值與最大值</b><br>
這也比MME得到的$2\bar{x}$還要實用 (不可能超過真實的上下界)</p>
<p><a href="https://www.projectrhea.org/rhea/index.php/Maximum_Likelihood_Estimation_Analysis_for_various_Probability_Distributions">常見分布MLE估計式，參考這裡</a><br>
<a href="https://online.stat.psu.edu/stat415/lesson/1/1.4">常見分布MME估計式，參考這裡</a></p>
<p>MLE 在大樣本下有很棒的性質，所以通常是不錯的估計方法<br>
(1) $\hat\theta$ 存在且唯一<br>
(2) 大樣本下滿足不偏性與有效性<br>
也就是MLE在大樣本下猜的很準，且答案一定只會有一個</p>
<p>三. 貝式估計式  (Bayes Estimator)</p>
<p>Def: 後驗分配(Posterior Distribution)<br>
$$f_{\theta|X}(\theta) = \frac{f(x1..xn|\theta)p(\theta)}{\int f(x1..xn|\theta)p(\theta)d\theta} \quad, p(\theta) 是先驗機率、f(x1..xn|\theta)是概似函數$$</p>
<p>後驗分配本身就式參考貝氏定理<br>
解釋I-數學式: 可以理解為分子是聯合分佈，分母是邊際分布(想想離散，把某一列全部加起來)<br>
解釋II-意義: 觀察到X(如:產品出現瑕疵)，求得到此情報後，機器故障的機率分布($f_{\theta|X}$)</p>
<p>Thm: 若 Xi 是iid 且服從 f(x,$\theta$)<br>
則貝氏估計式定義為<br>
求在限制式 Min $E_{\theta|x}[L(\hat{\theta};\theta)]$ 的 $\hat{\theta}$<br>
其中L為損失函數(loss function)，常見的有$\hat\theta$與$\theta$相減的絕對值或取平方</p>
<p>意義: 尋找期望上損失最小的估計參數$\hat{\theta}$</p>
<p>Thm: <b style="color:red; " >在損失函數是Square Loss下</b>(也就是${(\theta-\hat{\theta})}^2$)<br>
貝式估計式的參數解為 $\theta$乘上後驗函數的積分</p>
<p>$$\hat{\theta} = \int \theta f_{\theta|X}(\theta) d\theta$$</p>
<p>Example: 白努力分配<br>
Xi為iid 且服從白努力分配  $X_i|\theta$ ~ Bernoulli($\theta$)，並且$\theta$服從U(0,1)<br>
求以square error的貝式估計</p>
<p>$$f_{\theta|x1..xn}(\theta) = \frac{f(x1,..xn,\theta)}{f(x1,..,xn)} = \frac{f(x1,..xn|\theta)*f(\theta)}{\int f(x1,..,xn)d\theta}  $$</p>
<p>代入機率分布  $f(x1,..xn|\theta)$ 用白努力分配式，$f(\theta)$ 用均值分布此例中剛好均值分布是1  (1/b-a)</p>
<p>$$\frac{\Pi \theta^{x_i}{(1-\theta)}^{1-x_i}*1}{\int_0^1 {\theta}^{\sum{x_i}}{(1-\theta)}^{(n-\sum{x_i})}d\theta }$$</p>
<p>其中分母要代貝塔function，這裡就先不贅述，得答案為 樣本總和+1 / n+2</p>
<h1 id="好的估計式特性">好的估計式特性</h1>
<p>好的估計式特性:<br>
會具有以下四種特性:</p>
<ol>
<li>不偏性</li>
<li>有效性</li>
<li>充份性</li>
<li>完備性</li>
</ol>
<p>1.不偏性 (Unbiasedness)<br>
估計參數的期望值要等於母體參數<br>
即  $E(\hat{\theta}) = \theta$</p>
<p>Example: Bernoulli的MLE估計式具有不性<br>
$$\hat{p} = \bar{x} ==&gt;  E(\bar{x}) = \frac{\sum{E(x_i)}}{n} = \frac{np}{n} = p$$</p>
<p>2.有效性 (Efficiency)<br>
(不偏估計式中)Varience要最小<br>
即，給定兩個不偏估計式$\hat{{\theta}_1}$與$\hat{{\theta}_2}$<br>
若 Var($\hat{{\theta}_1}$) &lt; Var($\hat{{\theta}_2}$)，則我們說$\hat{{\theta}_1}$比較好</p>
<p>Thm: Cramer-Rao Lower Bound (C-R lower bound)<br>
$$Var(\hat{\theta}) &gt;= \frac{{\tau`(\theta)}^2}{n{*  E [\frac{\partial}{\partial \theta}lnf(x,\theta)] }^2 }$$</p>
<p>Note:</p>
<ol>
<li>通常$\tau(\theta) = \theta$，所以分子會變成1</li>
<li>定理用途:  只要求出不偏估計式，不偏估計式的variance又等於下界，則自動符合有效性</li>
</ol>
<p>以上兩點直觀意義: 估計本質上就是&quot;猜&rdquo;，我們希望猜的&quot;機率上最準&rdquo;(不偏性)，且不希望&quot;範圍太廣&rdquo;(有效性)<br>
(舉例來說，同樣滿足不偏性的情況下，猜中某一天發生地震，會比猜中某個月發生地震來的實用)</p>
<p>滿足這兩個性質被稱為 UMVUE (uniformly minimum-variance unbiased estimator)<br>
通常會用以下兩個性質幫助我們求解</p>
<p>3.充分性 (Sufficiency)<br>
重新分割樣本空間，使其對母體估計的訊息不少於原本的sample</p>
<p>說明: 分割樣本空間<br>
投擲三個銅板 <br>
其Sample Sapce = {(H,H,H),(T,H,H),(H,T,H),(H,H,T),(T,T,H),(T,H,T),(H,T,T),(T,T,T)} 共八種可能 <br>
若以H個個數來切割樣本空間，則會切割成四塊<br>
{<b style="color:red">(H,H,H)</b>,<b style="color:blue">(T,H,H),(H,T,H),(H,H,T)</b>,<b style="color:orange">(T,T,H),(T,H,T),(H,T,T)</b>,<b style="color:purple">(T,T,T)</b>}</p>
<p>以隨機變數的角度，就會是三個bernoulli x1,x2,x3，其隨機變數和T()會切割sample space為數個小集合<br>
這些小集合符合 T(x1,x2,x3)相等的特性</p>
<p>說明: 分割與資訊量<br>
這種分割可用於減少資料量(data reduction)，因為分割結果數比原本sample space結果數小<br>
(只需要記總和，就有和各個值一樣的效果)<br>
但又不能分割太少，否則會失去資訊量<br>
(比如投硬幣例子中只看前兩個結果)</p>
<p>充份性即說明不失去資訊量下的分割</p>
<p>數學定義:</p>
<p>$$T(X)是一個樣本分割，如果P(X|T(X)) 與母體參數\theta無關 ， 則T(X)是充份統計量 $$</p>
<p>解釋:</p>
<ol>
<li>所謂無關，就是不含$\theta$變數(對$\theta$是常數)</li>
<li>之所以這樣定義，代表 X 能靠估計方法得到的方程式解出的$\theta$ ， T(X)也可以做到</li>
</ol>
<p>Thm: 分解定理<br>
若pdf 可以拆成兩個部分乘積， 即與$\theta$、分割相關function 乘上 與$\theta$無關的function<br>
則 該分割就是充分統計量</p>
<p>即  $$f(X|\theta) = g(T(X),\theta)h(X) ， 則T(X)就是充分統計量$$</p>
<p>Note:</p>
<ol>
<li>計算上常常將h(X) = 1</li>
<li>此定理也告訴我們，充分統計量會有多個</li>
<li>充份統計量一定至少有自己、或是排序過資料(當然這樣就沒有簡化)</li>
</ol>
<p>Example: iid服從白努力分布的隨機變數和是充份統計量<br>
$$P(x1,..xn|\theta) = \Pi_{i=1,..n}{\theta}^{x_i}{1-\theta}^{1-x_i} = {\theta}^{\sum{x_i}}{1-\theta}^{n-\sum{x_i}} = {\theta}^{\sum{x_i}}{1-\theta}^{n-\sum{x_i}} *1 = g(T(X),\theta)h(X) $$</p>
<p>故得證，當然此題也可以用充分統計量定義計算</p>
<p>4.完備性 (Completeness)<br>
是針對<b >分佈</b>的特性，然後在此分布下導出的統計量具有完備性<br>
定義:<br>
如果一個分布是完備性的<br>
$$如果對所有\theta 都有 E[f(T(X))]=0 , 則f(T(X))=0 對所有 \theta都要成立 $$</p>
<p>Example:  Possion 具有完備性<br>
令 $x_i$ 是 iid 且服從 possion($\mu$)<br>
令 T = $\sum{x}$ 服從  possion(n$\mu$)</p>
<p>$$E[f(T(X))] = \sum_{x=0}^{\infty}(\frac{(n\mu)^xe^{-n\mu}}{x!}*f(T(X)) = 0  \quad by期望值定義$$</p>
<p>因為n$mu$ &gt; 0 , 整個式子要等於零，只有f(T(X))=0</p>
<p>故 E[f(T(X))] = 0 =&gt; f(T(X)) =0 對所有參數都成立，所以possion是完備的</p>
<!--   
圖片置中

<div style="text-align:center">
<img src= "git_book_1.PNG" style="width:400px;height:350px">
</div>
  
小節大標題
<h2 style="text-align:center; font-weight:bold; color:crimson" id="2-4"> 2-4 git遠端管理 </h2> 


紅字重點

<b style="color:red; " >problem 1 </b> 

--><blockquote>
</blockquote>

        </div>



        <footer class="post-footer">
                
                
                
        
                
                
                
        </footer>
        
<table cellspacing="15" style="width:100%; border: none;">
    <tr>
        <td style="text-align: center; border: none; padding: 0px;">
        </td>
        <td style="text-align: center; border: none; padding: 0px;">
        </td>
    </tr>
</table>

        
    </article>

    

</main>


        <div class="sidebar1 widgets columns-1">
    
    
    <div class="sidebar-item sidebar-pages">
    <p align="center" style="height:50%;
                margin:5px;
                width: 100%;
                height:100%"
                >
                    存放文章數:
                    53 </p>
     </div>

    
    <div class="sidebar-item sidebar-pages">
        <h3>連結</h3>
        <ul>
            
            <li>
                <a href="/">Home</a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-category">
        <h3>分類</h3>
        <ul>
            
            <li>
                <a href="/index/c_lang">C/C&#43;&#43;語言筆記</a>
                <br> ============= <br>
                     
            </li>
            
            <li>
                <a href="/index/device_driver">Device Driver系列</a>
                <br> ============= <br>
                     
            </li>
            
            <li>
                <a href="/index/web">網頁與爬蟲相關區</a>
                <br> ============= <br>
                     
            </li>
            
            <li>
                <a href="/index/course">課程筆記備份區</a>
                <br> ============= <br>
                     
            </li>
            
            <li>
                <a href="/index/misc">雜記區</a>
                <br> ============= <br>
                     
            </li>
            
            <li>
                <a href="/index/unix">Unix/Vim/bash區</a>
                <br> ============= <br>
                     
            </li>
            
            <li>
                <a href="/index/os">作業系統區</a>
                <br> ============= <br>
                     
            </li>
            
            <li>
                <a href="/index/machine_learning">機器學習區</a>
                <br> ============= <br>
                     
            </li>
            
            <li>
                <a href="/index/algorithm">演算法X資料結構區</a>
                <br> ============= <br>
                     
            </li>
            
            <li>
                <a href="/index/discretemath">離散數學</a>
                <br> ============= <br>
                     
            </li>
            
            <li>
                <a href="/index/hugo">hugo相關區</a>
                <br> ============= <br>
                     
            </li>
            
                   
                   
        </ul>
    </div>

    
</div>
        </div>
        </div>
<footer role="contentinfo" class="document-footer contentinfo widgets columns-1">

    <aside class="widget widget_text">
        <div class="textwidget">
            <p>© Little Cube`s box / Powered by <a href="https://gohugo.io" target="_blank">Hugo</a> with theme <a href="https://github.com/tosi29/inkblotty" target="_blank">Inkblotty</a></p>
        </div>
    </aside>
</footer>
</div>

    </body>
</html>
